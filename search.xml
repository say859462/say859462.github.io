<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Those games I&#39;ve ever played</title>
      <link href="/2025/07/07/Those-games-I-ve-ever-played/"/>
      <url>/2025/07/07/Those-games-I-ve-ever-played/</url>
      
        <content type="html"><![CDATA[<h3 id="Those-games-I-have-ever-played">Those games I have ever played</h3><span id="more"></span><table><thead><tr><th>Single-Player-Game</th><th style="text-align:center">Recommend</th></tr></thead><tbody><tr><td>Farthest Frontier</td><td style="text-align:center"></td></tr><tr><td>Sleeping Dogs</td><td style="text-align:center">✅</td></tr><tr><td>WWE 2K14</td><td style="text-align:center"></td></tr><tr><td>Ninja Gaiden II</td><td style="text-align:center"></td></tr><tr><td>Outlast</td><td style="text-align:center"></td></tr><tr><td>Assassin’s Creed IV: Black Flag</td><td style="text-align:center"></td></tr><tr><td>Assassin’s Creed Odyssey</td><td style="text-align:center">✅</td></tr><tr><td>Phoenix Wright: Ace Attorney Series</td><td style="text-align:center"></td></tr><tr><td>Danganronpa Series (1,2,V3)</td><td style="text-align:center">✅</td></tr><tr><td>Uncharted Series</td><td style="text-align:center"></td></tr><tr><td>Dave the Diver</td><td style="text-align:center"></td></tr><tr><td>Devil May Cry 5</td><td style="text-align:center"></td></tr><tr><td>Metaphor: ReFantazio</td><td style="text-align:center"></td></tr><tr><td>Sekiro: Shadows Die Twice</td><td style="text-align:center">✅</td></tr><tr><td>GTA V</td><td style="text-align:center"></td></tr><tr><td>Horizon Zero Dawn</td><td style="text-align:center"></td></tr><tr><td>Biohazard 8</td><td style="text-align:center"></td></tr><tr><td>Biohazard 6</td><td style="text-align:center"></td></tr><tr><td>Biohazard 4 Remake</td><td style="text-align:center"></td></tr><tr><td>Biohazard 3 Remake</td><td style="text-align:center"></td></tr><tr><td>Minecraft</td><td style="text-align:center"></td></tr><tr><td>TCG Card Shop Simulator</td><td style="text-align:center"></td></tr><tr><td>Stardew Valley</td><td style="text-align:center">✅</td></tr><tr><td>Spiritfarer</td><td style="text-align:center"></td></tr><tr><td>Beyond:Two Souls</td><td style="text-align:center"></td></tr><tr><td>Elden Ring</td><td style="text-align:center">✅</td></tr><tr><td>Timberborn</td><td style="text-align:center">✅</td></tr><tr><td>Crime Scene Cleaner</td><td style="text-align:center">✅</td></tr><tr><td>Tales of Arise</td><td style="text-align:center"></td></tr><tr><td>Sword Art Online: Hollow Realization</td><td style="text-align:center"></td></tr><tr><td>Animals Party</td><td style="text-align:center"></td></tr><tr><td>ENDER LILIES: Quietus of the Knights</td><td style="text-align:center">✅</td></tr><tr><td>Hollow Knight</td><td style="text-align:center"></td></tr><tr><td>Black Myth: Wukong</td><td style="text-align:center"></td></tr><tr><td>Red Dead Redemption</td><td style="text-align:center"></td></tr><tr><td>Red Dead Redemption 2</td><td style="text-align:center">✅</td></tr><tr><td>The Witcher 3</td><td style="text-align:center"></td></tr><tr><td>God of War 4</td><td style="text-align:center"></td></tr><tr><td>Two Point Hospital</td><td style="text-align:center"></td></tr><tr><td>Left 4 Dead 2</td><td style="text-align:center">✅</td></tr><tr><td>Frostpunk</td><td style="text-align:center"></td></tr><tr><td>Frostpunk 2</td><td style="text-align:center"></td></tr><tr><td>Palworld</td><td style="text-align:center"></td></tr><tr><td>HackNet</td><td style="text-align:center"></td></tr><tr><td>Cyber Manhunt</td><td style="text-align:center"></td></tr><tr><td>Cyberpunk 2077</td><td style="text-align:center"></td></tr><tr><td>Nine Sols</td><td style="text-align:center"></td></tr><tr><td>Final Fantasy XV</td><td style="text-align:center"></td></tr><tr><td>Final Fantasy VII reborn</td><td style="text-align:center"></td></tr><tr><td>Final Fantasy VII rebirth</td><td style="text-align:center"></td></tr><tr><td>LIGHTNING RETURNS: FINAL FANTASY XIII</td><td style="text-align:center"></td></tr><tr><td>Clair Obscur: Expedition 33</td><td style="text-align:center">✅</td></tr><tr><td>Story of Seasons: Grand Bazaar</td><td style="text-align:center"></td></tr></tbody></table><table><thead><tr><th>Online Games</th></tr></thead><tbody><tr><td>MixMaster</td></tr><tr><td>League of Legends</td></tr><tr><td>Apex</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> Games </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NYCU_2025_Summer_DL_Lab1</title>
      <link href="/2025/07/07/NYCU-2025-Summer-DL-Lab1/"/>
      <url>/2025/07/07/NYCU-2025-Summer-DL-Lab1/</url>
      
        <content type="html"><![CDATA[<h1>NYCU Deep Learning Lab-1: Back Propagation</h1><p>This is a Testing Post</p><hr><h2 id="Introduction">Introduction</h2><p>本次作業實作了一個簡單的深度神經網路，完成 Forward Propagation 與 Back Propagation 流程，以模擬模型的訓練過程。同時加入多種 Activation Function 與 Optimizer，觀察它們對模型收斂與最終準確率的影響。</p><p>訓練資料使用助教提供的 Linear Dataset 與 XOR Dataset，透過不同模型結構與超參數調整，更深入理解 Back Propagation 運作原理及深度學習中梯度下降的概念。</p><hr><span id="more"></span><h2 id="Implement-Detail">Implement Detail</h2><h3 id="Network-Architecture">Network Architecture</h3><ul><li>Input Layer: 2 個節點</li><li>Hidden Layer: 兩層，可調整單元數</li><li>Output Layer: 1 個節點</li><li>結構: 全連接 (fully connected)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_size=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">        output_size=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">        hidden_layers_size=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">        activation=<span class="string">&quot;sigmoid&quot;</span>,</span></span><br><span class="line"><span class="params">        optimizer=<span class="string">&quot;SGD&quot;</span>,</span></span><br><span class="line"><span class="params">        learning_rate=<span class="number">0.01</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="variable language_">self</span>.losses = []</span><br><span class="line">        <span class="variable language_">self</span>.layers = []</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate</span><br><span class="line">        <span class="comment"># Build model</span></span><br><span class="line">        <span class="variable language_">self</span>.layers.append(</span><br><span class="line">            Linear_Layer(input_size, hidden_layers_size, activation, optimizer)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.layers.append(</span><br><span class="line">            Linear_Layer(hidden_layers_size, hidden_layers_size, activation, optimizer)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.layers.append(</span><br><span class="line">            Linear_Layer(hidden_layers_size, output_size, activation, optimizer)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><h3 id="Activation-Functions">Activation Functions</h3><ul><li>Sigmoid</li><li>ReLU</li><li>Tanh</li><li>均實作微分，用於梯度計算</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Activative functions and their derivatives</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">derivative_sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.multiply(x, <span class="number">1.0</span> - x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">derivative_relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x &gt;= <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">derivative_tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - np.square(np.tanh(x))</span><br></pre></td></tr></table></figure><h3 id="Backpropagation">Backpropagation</h3><ul><li>損失函數: Mean Squared Error (MSE)</li><li>計算各層梯度，更新 weights 與 bias</li><li>根據 chain rule 傳遞 δ 值，計算 dW 與 db</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, upstream_delta, learning_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="comment"># check whether the activation function is empty</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.activation:</span><br><span class="line">            delta = upstream_delta</span><br><span class="line">            <span class="comment"># If no activation function, delta is just upstream_delta</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            delta = upstream_delta * derivative_activation_map[<span class="variable language_">self</span>.activation](<span class="variable language_">self</span>.a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate gradients of weights and bias</span></span><br><span class="line">        dW = np.dot(<span class="variable language_">self</span>.<span class="built_in">input</span>.T, delta)</span><br><span class="line">        db = np.<span class="built_in">sum</span>(delta)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update weights and bias</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.optimizer == <span class="string">&quot;SGD&quot;</span>:</span><br><span class="line">            <span class="variable language_">self</span>.weights -= dW * learning_rate</span><br><span class="line">            <span class="variable language_">self</span>.bias -= db * learning_rate</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.optimizer == <span class="string">&quot;Adagrad&quot;</span>:</span><br><span class="line">            <span class="comment"># For the purpose of best training, learning rate should be adjusted according to the gradients</span></span><br><span class="line">            <span class="comment"># If gradients are small, learning rate should be larger, vice versa</span></span><br><span class="line">            <span class="variable language_">self</span>.total_grad_w += np.square(dW)</span><br><span class="line">            <span class="variable language_">self</span>.total_grad_b += np.square(db)</span><br><span class="line">            <span class="variable language_">self</span>.weights -= (</span><br><span class="line">                dW * learning_rate / np.sqrt(<span class="variable language_">self</span>.total_grad_w + <span class="variable language_">self</span>.epsilon)</span><br><span class="line">            )</span><br><span class="line">            <span class="variable language_">self</span>.bias -= db * learning_rate / np.sqrt(<span class="variable language_">self</span>.total_grad_b + <span class="variable language_">self</span>.epsilon)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.optimizer == <span class="string">&quot;Momentum&quot;</span>:</span><br><span class="line">            <span class="variable language_">self</span>.v_weight = <span class="variable language_">self</span>.momentum * <span class="variable language_">self</span>.v_weight + dW * learning_rate</span><br><span class="line">            <span class="variable language_">self</span>.v_bias = <span class="variable language_">self</span>.momentum * <span class="variable language_">self</span>.v_bias + db * learning_rate</span><br><span class="line">            <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.v_weight</span><br><span class="line">            <span class="variable language_">self</span>.bias -= <span class="variable language_">self</span>.v_bias</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.dot(delta, <span class="variable language_">self</span>.weights.T)  <span class="comment"># Return delta for the previous layer</span></span><br></pre></td></tr></table></figure><h3 id="Extra-Implementation">Extra Implementation</h3><ul><li>實作 Optimizer: SGD、Momentum、Adagrad</li><li>可調整參數: Learning rate、hidden units 數量、Activation Function</li></ul><hr><h2 id="Experimental-Results">Experimental Results</h2><h3 id="Screenshot-Comparison-Figures">Screenshot &amp; Comparison Figures</h3><ul><li>Linear Dataset 與 XOR Dataset 訓練結果截圖</li></ul><h3 id="Prediction-Accuracy">Prediction Accuracy</h3><table><thead><tr><th>Dataset</th><th>Activation</th><th>Optimizer</th><th>LR</th><th>Hidden Units</th><th>Accuracy</th><th>Result</th></tr></thead><tbody><tr><td>Linear</td><td>tanh</td><td>SGD</td><td>0.01</td><td>10</td><td>100%</td><td><img src="/images/Linear_Acc.png" alt="Linear Accuracy"></td></tr><tr><td>XOR</td><td>tanh</td><td>SGD</td><td>0.01</td><td>10</td><td>100%</td><td><img src="/images/XOR_Acc.png" alt="XOR Accuracy"></td></tr></tbody></table><h3 id="Learning-Curves">Learning Curves</h3><ul><li>顯示 Linear 與 XOR Dataset 在不同條件下的 loss-epoch 曲線</li></ul><table><thead><tr><th>Linear</th><th>XOR</th></tr></thead><tbody><tr><td><img src="/images/Linear_Learning_Curve.png" alt="Linear Learning Curve"></td><td><img src="/images/XOR_Learning_Curve.png" alt="XOR Learning Curve"></td></tr></tbody></table><hr><h2 id="Discussions">Discussions</h2><h3 id="Learning-Rate">Learning Rate</h3><ul><li>Linear Dataset: 不同 learning rate 對最終準確率影響不大，但影響收斂速度。</li><li>XOR Dataset: learning rate 小導致 loss 緩慢下降且最終準確率低，learning rate 大則 loss 快速下降並可達到 100% 準確率。</li></ul><h3 id="Hidden-Units">Hidden Units</h3><ul><li>Linear Dataset: 增加 hidden units 數量收斂更穩定，但差異不大。</li><li>XOR Dataset: 增加 hidden units 數量後 loss 明顯下降，準確率明顯提升。</li></ul><h3 id="Without-Activation-Functions">Without Activation Functions</h3><ul><li>Linear Dataset: 收斂不穩定且準確率稍降。</li><li>XOR Dataset: 幾乎無法訓練成功。</li></ul><h3 id="Optimizer-比較">Optimizer 比較</h3><ul><li>Linear Dataset: Momentum 與 Adagrad 收斂速度優於單純 SGD。</li><li>XOR Dataset: SGD 無法成功訓練，Momentum 與 Adagrad 能讓準確率達到 100%。</li></ul><h3 id="Activation-Functions-比較">Activation Functions 比較</h3><ul><li>Linear Dataset &amp; XOR Dataset: tanh 表現最佳，sigmoid 其次，relu 無法訓練。</li></ul><hr><h2 id="Questions">Questions</h2><p><strong>A. Activation Functions 的作用</strong><br>提供非線性效果，使神經網路能近似複雜函數。</p><p><strong>B. Learning Rate 過大或過小</strong><br>太小：收斂慢；太大：可能發散。</p><p><strong>C. Weights &amp; Biases 的作用</strong><br>決定輸入與輸出關係與模型彈性，使模型能夠擬合數據分布。</p><hr><h2 id="Reference">Reference</h2><ul><li><a href="https://datasciocean.tech/deep-learning-core-concept/backpropagation-explain/">https://datasciocean.tech/deep-learning-core-concept/backpropagation-explain/</a></li><li><a href="https://medium.com/.../ml-note-sgd-momentum-adagrad-adam-optimizer-f20568c968db">https://medium.com/.../ml-note-sgd-momentum-adagrad-adam-optimizer-f20568c968db</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NYCU </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NYCU_2025_Summer_DL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
